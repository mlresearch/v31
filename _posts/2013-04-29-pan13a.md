---
pdf: http://proceedings.mlr.press/v31/pan13a.pdf
supplementary: http://proceedings.mlr.press/v31/pan13a-supp.pdf
title: High-dimensional Inference via Lipschitz Sparsity-Yielding Regularizers
abstract: Non-convex regularizers are more and more applied to high-dimensional inference
  with sparsity prior knowledge. In general, the non-convex regularizer is superior
  to the convex ones in inference but it suffers the difficulties brought by local
  optimums and massive computation. A "good" regularizer should perform well in both
  inference and optimization. In this paper, we prove that some non-convex regularizers
  can be such "good" regularizers. They are a family of sparsity-yielding penalties
  with proper Lipschitz subgradients. These regularizers keep the superiority of non-convex
  regularizers in inference. Their estimation conditions based on sparse eigenvalues
  are weaker than the convex regularizers. Meanwhile, if properly tuned, they behave
  like convex regularizers since standard proximal methods guarantee to give stationary
  solutions. These stationary solutions, if sparse enough, are identical to the global
  solutions. If the solution sequence provided by proximal methods is along a sparse
  path, the convergence rate to the global optimum is on the order of 1/k where k
  is the number of iterations.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pan13a
month: 0
tex_title: High-dimensional Inference via Lipschitz Sparsity-Yielding Regularizers
firstpage: 481
lastpage: 488
page: 481-488
order: 481
cycles: false
author:
- given: Zheng
  family: Pan
- given: Changshui
  family: Zhang
date: 2013-04-29
address: Scottsdale, Arizona, USA
publisher: PMLR
container-title: Proceedings of the Sixteenth International Conference on Artificial
  Intelligence and Statistics
volume: '31'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 4
  - 29
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
