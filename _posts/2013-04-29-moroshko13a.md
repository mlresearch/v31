---
pdf: http://proceedings.mlr.press/v31/moroshko13a.pdf
title: A Last-Step Regression Algorithm for Non-Stationary Online Learning
abstract: The goal of a learner in standard online learning is to maintain an average
  loss close to the loss of the best-performing single function in some class. In
  many real-world problems, such as rating or ranking items, there is no single best
  target function during the runtime of the algorithm, instead the best (local) target
  function is drifting over time. We develop a novel last step minmax optimal algorithm
  in context of a drift. We analyze the algorithm in the worst-case regret framework
  and show that it maintains an average loss close to that of the best slowly changing
  sequence of linear functions, as long as the total of drift is sublinear. In some
  situations, our bound improves over existing bounds, and additionally the algorithm
  suffers logarithmic regret when there is no drift. We also build on the H1 filter
  and its bound, and develop and analyze a second algorithm for drifting setting.
  Synthetic simulations demonstrate the advantages of our algorithms in a worst-case
  constant drift setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: moroshko13a
month: 0
tex_title: A Last-Step Regression Algorithm for Non-Stationary Online Learning
firstpage: 451
lastpage: 462
page: 451-462
sections: 
author:
- given: Edward
  family: Moroshko
- given: Koby
  family: Crammer
date: 2013-04-29
address: Scottsdale, Arizona, USA
publisher: PMLR
container-title: Proceedings of the Sixteenth International Conference on Artificial
  Intelligence and Statistics
volume: '31'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 4
  - 29
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
