---
pdf: http://proceedings.mlr.press/v31/damianou13a.pdf
title: Deep Gaussian Processes
abstract: In this paper we introduce deep Gaussian process (GP) models. Deep GPs are
  a deep belief network based on Gaussian process mappings. The data is modeled as
  the output of a multivariate GP. The inputs to that Gaussian process are then governed
  by another GP. A single layer model is equivalent to a standard GP or the GP latent
  variable model (GP-LVM). We perform inference in the model by approximate variational
  marginalization. This results in a strict lower bound on the marginal likelihood
  of the model which we use for model selection (number of layers and nodes per layer).
  Deep belief networks are typically applied to relatively large data sets using stochastic
  gradient descent for optimization. Our fully Bayesian treatment allows for the application
  of deep models even when data is scarce. Model selection by our variational bound
  shows that a five layer hierarchy is justified even when modelling a digit data
  set containing only 150 examples.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: damianou13a
month: 0
tex_title: Deep Gaussian Processes
firstpage: 207
lastpage: 215
page: 207-215
sections: 
author:
- given: Andreas
  family: Damianou
- given: Neil
  family: Lawrence
date: 2013-04-29
address: Scottsdale, Arizona, USA
publisher: PMLR
container-title: Proceedings of the Sixteenth International Conference on Artificial
  Intelligence and Statistics
volume: '31'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 4
  - 29
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
