<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Further Optimal Regret Bounds for Thompson Sampling | AISTATS 2013 | JMLR W&amp;CP</title>

		<!-- Stylesheet -->
		<link rel="stylesheet" type="text/css" href="../css/jmlr.css">

		<!-- MathJax -->
		<script type="text/x-mathjax-config">
  			MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<!-- Metadata -->
		<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Further Optimal Regret Bounds for Thompson Sampling">
<meta name="citation_author" content="Agrawal, Shipra">
<meta name="citation_author" content="Goyal, Navin">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="99">
<meta name="citation_lastpage" content="107">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v31/agrawal13a.pdf">


    </head>
    <body>

	<div id="fixed">
<a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="../img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="../img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
	</div>

	<div id="content">
		<h1>Further Optimal Regret Bounds for Thompson Sampling</h1>

<div id="authors">Shipra Agrawal, Navin Goyal</div>;
<div id="info">JMLR W&amp;CP 31 : 99–107, 2013</div>



<h2>Abstract</h2>
<div id="abstract">
	Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have comparable or better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that proves the first near-optimal problem-independent bound of <span class="math">\(O(\sqrt{NT\ln T})\)</span> on the expected regret of this algorithm. Our novel martingale-based analysis techniques are conceptually simple, and easily extend to distributions other than the Beta distribution. For the version of Thompson Sampling that uses Gaussian priors, we prove a problem-independent bound of <span class="math">\(O(\sqrt{NT\ln N})\)</span> on the expected regret, and demonstrate the optimality of this bound by providing a matching lower bound. This lower bound of <span class="math">\(\Omega(\sqrt{NT\ln N})\)</span> is the first lower bound on the performance of a natural version of Thompson Sampling that is away from the general lower bound of <span class="math">\(O(\sqrt{NT})\)</span> for the multi-armed bandit problem. Our near-optimal problem-independent bounds for Thompson Sampling solve a COLT 2012 open problem of Chapelle and Li. Additionally, our techniques simultaneously provide the optimal problem-dependent bound of <span class="math">\((1+\epsilon)\sum_i \frac{\ln T}{d(\mu_i, \mu_1)}+O(\frac{N}{\epsilon^2})\)</span> on the expected regret. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [2012].
</div>

<h2>Related Material</h2>
<div id="extras">
	<ul>
		<li><a href="agrawal13a.pdf">Download PDF</a></li>
		<li><a href="agrawal13a-supp.pdf">Supplementary (PDF)</a></li>

	</ul>
</div>

	</div>

    </body>
</html>
