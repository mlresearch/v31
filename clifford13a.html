<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>A simple sketching algorithm for entropy estimation over streaming data | AISTATS 2013 | JMLR W&amp;CP</title>

		<!-- Stylesheet -->
		<link rel="stylesheet" type="text/css" href="../css/jmlr.css">

		<!-- MathJax -->
		<script type="text/x-mathjax-config">
  			MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<!-- Metadata -->
		<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="A simple sketching algorithm for entropy estimation over streaming data">
<meta name="citation_author" content="Clifford, Peter">
<meta name="citation_author" content="Cosma, Ioana">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="196">
<meta name="citation_lastpage" content="206">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v31/clifford13a.pdf">


    </head>
    <body>

	<div id="fixed">
<a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="../img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="../img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
	</div>

	<div id="content">
		<h1>A simple sketching algorithm for entropy estimation over streaming data</h1>

<div id="authors">Peter Clifford, Ioana Cosma</div>;
<div id="info">JMLR W&amp;CP 31 : 196–206, 2013</div>



<h2>Abstract</h2>
<div id="abstract">
	We consider the problem of approximating the empirical Shannon entropy of a high-frequency data stream under the relaxed strict-turnstile model, when space limitations make exact computation infeasible. An equivalent measure of entropy is the Renyi entropy that depends on a constant <span class="math">\(\alpha\)</span>. This quantity can be estimated efficiently and unbiasedly from a low-dimensional synopsis called an <span class="math">\(\alpha\)</span>-stable data sketch via the method of compressed counting. An approximation to the Shannon entropy can be obtained from the Renyi entropy by taking alpha sufficiently close to 1. However, practical guidelines for parameter calibration with respect to <span class="math">\(\alpha\)</span> are lacking. We avoid this problem by showing that the random variables used in estimating the Renyi entropy can be transformed to have a proper distributional limit as <span class="math">\(\alpha\)</span> approaches 1: the maximally skewed, strictly stable distribution with <span class="math">\(\alpha = 1\)</span> defined on the entire real line. We propose a family of asymptotically unbiased log-mean estimators of the Shannon entropy, indexed by a constant <span class="math">\(\zeta &gt; 0\)</span>, that can be computed in a single-pass algorithm to provide an additive approximation. We recommend the log-mean estimator with <span class="math">\(\zeta = 1\)</span> that has exponentially decreasing tail bounds on the error probability, asymptotic relative efficiency of 0.932, and near-optimal computational complexity.
</div>

<h2>Related Material</h2>
<div id="extras">
	<ul>
		<li><a href="clifford13a.pdf">Download PDF</a></li>
		
	</ul>
</div>

	</div>

    </body>
</html>
